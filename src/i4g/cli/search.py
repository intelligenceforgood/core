"""Search-oriented CLI helpers (RAG + Vertex)."""

from __future__ import annotations

import json
import logging
import subprocess
import sys
from collections.abc import Sequence as AbcSequence
from dataclasses import dataclass
from pathlib import Path
from typing import Any
from urllib.error import HTTPError, URLError
from urllib.request import Request, urlopen

from google.cloud import discoveryengine_v1beta as discoveryengine
from google.protobuf import json_format

from i4g.cli.utils import SETTINGS, console
from i4g.services.factories import build_vector_store


def _convert_struct(data: Any) -> Any:
    """Recursively convert proto-plus containers into builtin types."""

    if isinstance(data, (str, int, float, bool)) or data is None:
        return data
    if hasattr(data, "items"):
        return {key: _convert_struct(value) for key, value in data.items()}
    if isinstance(data, AbcSequence) and not isinstance(data, (str, bytes, bytearray)):
        return [_convert_struct(value) for value in data]
    return data


def ensure_ollama_running() -> bool:
    """Quick connectivity check for Ollama."""

    try:
        result = subprocess.run(["ollama", "list"], capture_output=True, text=True, timeout=3)
        return result.returncode == 0
    except Exception:
        return False


def run_query(args: Any) -> None:
    """Execute a RAG scam-detection query using the configured vector backend."""

    from i4g.rag.pipeline import build_scam_detection_chain

    if not ensure_ollama_running():
        console.print("[red]‚ùå Ollama is not running. Start it first with:[/red]")
        console.print("    ollama serve\n")
        sys.exit(1)

    console.print("[green]‚úÖ Ollama detected. Loading vectorstore...[/green]")

    try:
        store = build_vector_store(backend=args.backend)
    except Exception as exc:  # pragma: no cover - defensive logging
        console.print(f"[red]Failed to initialize vectorstore:[/red] {exc}")
        console.print("Make sure you ran `i4g data build-index` successfully.")
        sys.exit(1)

    console.print("[cyan]üîó Building scam detection chain...[/cyan]")
    chain = build_scam_detection_chain(store)

    console.print(f"[cyan]ü§ñ Analyzing question:[/cyan] {args.question}\n")

    try:
        result = chain.invoke({"question": args.question})
    except Exception as exc:  # pragma: no cover - defensive logging
        console.print(f"[red]‚ùå Query failed:[/red] {exc}")
        sys.exit(1)

    console.print("\n[bold green]üß† Scam Detection Result:[/bold green]")
    console.print(result)
    console.print("\n[dim]Note: Results are generated by a local LLM using RAG context retrieval.[/dim]")


def run_vertex_search(args: Any) -> None:
    """Run a Discovery search and print either raw JSON or a summary."""

    project = args.project or SETTINGS.vector.vertex_ai_project
    if not project:
        console.print("[red]‚ùå Provide --project or set I4G_VECTOR__VERTEX_AI__PROJECT.[/red]")
        sys.exit(1)

    location = args.location or SETTINGS.vector.vertex_ai_location or "global"
    data_store_id = args.data_store_id
    if not data_store_id:
        console.print("[red]‚ùå --data-store-id is required.[/red]")
        sys.exit(1)

    client = discoveryengine.SearchServiceClient()
    serving_config = client.serving_config_path(
        project=project,
        location=location,
        data_store=data_store_id,
        serving_config=args.serving_config_id,
    )

    request = discoveryengine.SearchRequest(
        serving_config=serving_config,
        query=args.query,
        page_size=args.page_size,
    )

    if args.filter_expression:
        request.filter = args.filter_expression

    if args.boost_json:
        try:
            boost_payload = json.loads(args.boost_json)
        except json.JSONDecodeError as exc:
            console.print(f"[red]‚ùå Failed to parse --boost-json:[/red] {exc}")
            sys.exit(1)

        boost_spec = discoveryengine.SearchRequest.BoostSpec()
        json_format.ParseDict(boost_payload, boost_spec._pb)
        request.boost_spec = boost_spec

    console.print(
        f"[cyan]üîç Vertex search:[/cyan] project={project} location={location} data_store={data_store_id} query='{args.query}'"
    )

    try:
        results = list(client.search(request=request))
    except Exception as exc:  # pragma: no cover - network failure
        console.print(f"[red]‚ùå Search failed:[/red] {exc}")
        sys.exit(1)

    if args.raw:
        payload = [json_format.MessageToDict(result._pb) for result in results]  # type: ignore[attr-defined]
        console.print_json(data=json.dumps(payload))
        return

    if not results:
        console.print("[yellow]No results returned.[/yellow]")
        return

    for rank, result in enumerate(results, start=1):
        document = result.document
        struct: dict[str, Any] = {}
        if document.json_data:
            try:
                struct = json.loads(document.json_data)
            except json.JSONDecodeError:
                struct = _convert_struct(document.struct_data) if document.struct_data else {}
        elif document.struct_data:
            struct = _convert_struct(document.struct_data)

        summary = struct.get("summary") or document.title or "<no summary>"
        label = struct.get("ground_truth_label") or "<unknown>"
        tags = ", ".join(struct.get("tags") or [])

        console.print(f"[bold cyan]#{rank}[/bold cyan] id={document.id} label={label}")
        if tags:
            console.print(f"    [dim]tags:[/dim] {tags}")
        console.print(f"    {summary}\n")


def query_vertex(args: Any) -> None:
    """Run an ad-hoc Vertex AI Search query and print a summary or raw JSON."""

    client = discoveryengine.SearchServiceClient()
    serving_config = client.serving_config_path(
        project=args.project,
        location=args.location,
        data_store=args.data_store_id,
        serving_config=args.serving_config_id,
    )

    request = discoveryengine.SearchRequest(
        serving_config=serving_config,
        query=args.query,
        page_size=args.page_size,
    )

    if args.filter_expression:
        request.filter = args.filter_expression

    if args.boost_json:
        try:
            boost_payload = json.loads(args.boost_json)
        except json.JSONDecodeError as exc:
            raise SystemExit(f"Failed to parse --boost-json payload: {exc}") from exc

        boost_spec = discoveryengine.SearchRequest.BoostSpec()
        json_format.ParseDict(boost_payload, boost_spec._pb)
        request.boost_spec = boost_spec

    results_iter = client.search(request=request)
    if args.page_size and args.page_size > 0:
        results = list(results_iter)[: args.page_size]
    else:
        results = list(results_iter)

    if args.raw:
        payload = [json_format.MessageToDict(result._pb) for result in results]  # type: ignore[attr-defined]
        console.print_json(data=json.dumps(payload))
        return

    for rank, result in enumerate(results, start=1):
        document = result.document
        struct: dict[str, Any] = {}
        if document.json_data:
            try:
                struct = json.loads(document.json_data)
            except json.JSONDecodeError:
                logging.debug("Failed to decode json_data for document %s", document.id)
        elif document.struct_data:
            struct = _convert_struct(document.struct_data)

        summary = (
            struct.get("summary")
            or struct.get("subject")
            or struct.get("title")
            or (struct.get("content", "")[:120] + "‚Ä¶" if struct.get("content") else None)
            or "<no summary>"
        )
        console.print(f"[bold cyan]#{rank}[/bold cyan] id={document.id}")
        console.print(f"    {summary}")
        tags = struct.get("tags")
        if isinstance(tags, list) and tags:
            console.print(f"    [dim]tags:[/dim] {', '.join(tags)}")
        source = struct.get("source")
        index_type = struct.get("index_type")
        meta_parts: list[str] = []
        if isinstance(source, str) and source:
            meta_parts.append(f"source={source}")
        if isinstance(index_type, str) and index_type and index_type != source:
            meta_parts.append(f"index_type={index_type}")
        if meta_parts:
            console.print(f"    [dim]meta:[/dim] {', '.join(meta_parts)}")
        console.print()


@dataclass
class Scenario:
    name: str
    query: str
    filter_expression: str | None = None
    boost_json: str | None = None
    page_size: int = 10
    expected_ids: list[str] | None = None
    expected_labels: list[str] | None = None
    expected_tags: list[str] | None = None
    pass_k: int = 3

    @classmethod
    def from_dict(cls, payload: dict[str, Any]) -> "Scenario":
        expected_ids = payload.get("expected_ids")
        expected_labels = payload.get("expected_labels")
        expected_tags = payload.get("expected_tags")

        return cls(
            name=payload["name"],
            query=payload["query"],
            filter_expression=payload.get("filter") or payload.get("filter_expression"),
            boost_json=payload.get("boost_json"),
            page_size=int(payload.get("page_size", 10)),
            expected_ids=list(expected_ids) if expected_ids else None,
            expected_labels=list(expected_labels) if expected_labels else None,
            expected_tags=list(expected_tags) if expected_tags else None,
            pass_k=int(payload.get("pass_k", 3)),
        )


DEFAULT_SCENARIOS: list[Scenario] = [
    Scenario(
        name="Wallet verification spike",
        query="suspicious withdrawal",
        filter_expression='tags: ANY("account-security")',
        expected_labels=["wallet_verification"],
        pass_k=5,
        page_size=10,
    ),
    Scenario(
        name="Romance visa pretext",
        query="immigration fee",
        filter_expression='tags: ANY("romance")',
        expected_labels=["romance_bitcoin"],
        pass_k=5,
        page_size=10,
    ),
    Scenario(
        name="Investment pump room",
        query="liquidity injection on radiant",
        filter_expression='tags: ANY("investment")',
        expected_labels=["investment_group"],
        pass_k=5,
        page_size=10,
    ),
    Scenario(
        name="Tech support remote access",
        query="license expired teamviewer",
        filter_expression='tags: ANY("tech_support_scam")',
        expected_labels=["tech_support"],
        pass_k=5,
        page_size=10,
    ),
]


def load_scenarios(path: Path | None) -> list[Scenario]:
    if path is None:
        return DEFAULT_SCENARIOS

    payload = json.loads(path.read_text(encoding="utf-8"))
    if not isinstance(payload, list):
        raise SystemExit("Scenario config must be a list of objects.")
    return [Scenario.from_dict(entry) for entry in payload]


def evaluate_vertex(args: Any) -> int:
    logging.basicConfig(level=logging.DEBUG if args.verbose else logging.INFO, format="%(levelname)s %(message)s")
    scenarios = load_scenarios(Path(args.config) if args.config else None)
    client = discoveryengine.SearchServiceClient()

    total = len(scenarios)
    passes = 0
    worst_rank: int | None = None

    for scenario in scenarios:
        request = discoveryengine.SearchRequest(
            serving_config=discoveryengine.SearchServiceClient.serving_config_path(
                project=args.project,
                location=args.location,
                data_store=args.data_store_id,
                serving_config=args.serving_config_id,
            ),
            query=scenario.query,
            page_size=scenario.page_size,
        )
        if scenario.filter_expression:
            request.filter = scenario.filter_expression
        if scenario.boost_json:
            boost_spec = discoveryengine.SearchRequest.BoostSpec()
            json_format.ParseDict(json.loads(scenario.boost_json), boost_spec._pb)
            request.boost_spec = boost_spec

        results: list[dict[str, Any]] = []
        for index, result in enumerate(client.search(request=request), start=1):
            document = result.document
            struct: dict[str, Any] = {}
            if document.json_data:
                try:
                    struct = json.loads(document.json_data)
                except json.JSONDecodeError:
                    logging.debug("Failed to decode json_data for document %s", document.id)
            elif document.struct_data:
                struct = json_format.MessageToDict(document.struct_data._pb)

            tags = struct.get("tags") if isinstance(struct.get("tags"), list) else []
            label = struct.get("ground_truth_label")
            results.append(
                {
                    "rank": index,
                    "id": document.id,
                    "summary": struct.get("summary") or document.title,
                    "label": label,
                    "tags": tags,
                }
            )

        match_rank: int | None = None
        for item in results:
            rank = item["rank"]
            doc_id = item["id"]
            label = item.get("label")
            tags = item.get("tags") or []

            matches_id = scenario.expected_ids and doc_id in scenario.expected_ids
            matches_label = scenario.expected_labels and label in scenario.expected_labels
            matches_tags = scenario.expected_tags and any(tag in scenario.expected_tags for tag in tags)

            if matches_id or matches_label or matches_tags:
                match_rank = rank
                break

        passed = match_rank is not None and match_rank <= scenario.pass_k
        indicator = "‚úÖ" if passed else "‚ùå"
        console.print(f"{indicator} {'PASS' if passed else 'FAIL'} ‚Äî {scenario.name}")
        if match_rank is None:
            console.print(f"    No matching document found in top {len(results)} results.")
        else:
            console.print(f"    First match at rank {match_rank}; pass_k={scenario.pass_k}")
            worst_rank = max(worst_rank or match_rank, match_rank)

        for item in results[: scenario.pass_k]:
            tags = ", ".join(item.get("tags") or [])
            label = item.get("label") or "<unknown>"
            summary = item.get("summary") or "<no summary>"
            console.print(f"      #{item['rank']}: id={item['id']} label={label} tags={tags}")
            console.print(f"           {summary}")
        console.print()

        if passed:
            passes += 1

    console.print(f"Summary: {passes}/{total} scenarios passed")
    if worst_rank is not None:
        console.print(f"Worst passing rank: {worst_rank}")

    return 0 if passes == total else 1


def refresh_hybrid_schema_snapshot(args: Any) -> None:
    """Fetch `/reviews/search/schema` and write a local snapshot."""

    url = f"{args.api_base.rstrip('/')}/reviews/search/schema"
    request = Request(url)
    request.add_header("Accept", "application/json")
    if args.api_key:
        request.add_header("X-API-KEY", args.api_key)

    try:
        with urlopen(request, timeout=args.timeout) as response:  # noqa: S310
            data = response.read()
    except HTTPError as exc:  # pragma: no cover - exercised in integration flows
        raise RuntimeError(f"Schema request failed with status {exc.code}: {exc.reason}") from exc
    except URLError as exc:  # pragma: no cover - exercised in integration flows
        raise RuntimeError(f"Unable to reach schema endpoint: {exc.reason}") from exc

    payload = json.loads(data)
    output = Path(args.output)
    output.parent.mkdir(parents=True, exist_ok=True)
    output.write_text(json.dumps(payload, indent=args.indent) + "\n", encoding="utf-8")
    console.print(f"[green]‚úÖ Wrote schema snapshot to {output}[/green]")


__all__ = [
    "run_query",
    "run_vertex_search",
    "query_vertex",
    "evaluate_vertex",
    "refresh_hybrid_schema_snapshot",
    "ensure_ollama_running",
]
